import boto3
import pandas as pd
import logging
from botocore.exceptions import ClientError
from typing import List

class S3Manager:
    def __init__(self, bucket_name: str, region: str, access_key_id: str, secret_access_key: str):
        self.bucket_name = bucket_name
        try:
            self.s3_client = boto3.client(
                's3',
                region_name=region,
                aws_access_key_id=access_key_id,
                aws_secret_access_key=secret_access_key
            )
            self.ensure_bucket_exists()
        except Exception as e:
            logging.error(f"Failed to initialize S3 client: {str(e)}")
            raise
    
    def ensure_bucket_exists(self):
        try:
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            logging.info(f"S3 bucket {self.bucket_name} already exists")
        except self.s3_client.exceptions.ClientError as e:
            if e.response['Error']['Code'] == '404':
                try:
                    self.s3_client.create_bucket(Bucket=self.bucket_name)
                    logging.info(f"Created S3 bucket: {self.bucket_name}")
                except Exception as e:
                    logging.error(f"Failed to create S3 bucket: {str(e)}")
                    raise
            elif e.response['Error']['Code'] == '403':
                logging.error("Access denied to S3. Check AWS credentials or permissions.")
                raise
            else:
                logging.error(f"Failed to check S3 bucket existence: {str(e)}")
                raise
        except Exception as e:
            logging.error(f"Unexpected error checking S3 bucket: {str(e)}")
            raise
    
    def upload_dataframe(self, df: pd.DataFrame, s3_key: str, retries: int = 3) -> bool:
        for attempt in range(1, retries + 1):
            try:
                csv_buffer = df.to_csv(index=False)
                self.s3_client.put_object(
                    Bucket=self.bucket_name,
                    Key=s3_key,
                    Body=csv_buffer
                )
                self.s3_client.head_object(Bucket=self.bucket_name, Key=s3_key)
                logging.info(f"Successfully uploaded {s3_key} to S3")
                return True
            except ClientError as e:
                logging.error(f"Attempt {attempt}/{retries} failed to upload {s3_key}: {str(e)}")
                if attempt == retries:
                    logging.error(f"Failed to upload {s3_key} after {retries} attempts")
                    return False
                time.sleep(2 ** attempt)
            except Exception as e:
                logging.error(f"Unexpected error uploading {s3_key}: {str(e)}")
                return False
        return False
    
    def list_new_files(self, prefix: str, processed_files: set) -> List[str]:
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=prefix
            )
            
            if 'Contents' not in response:
                return []
            
            new_files = []
            for obj in response['Contents']:
                if obj['Key'] not in processed_files:
                    new_files.append(obj['Key'])
            
            return new_files
        except Exception as e:
            logging.error(f"Failed to list S3 files: {str(e)}")
            return []
    
    def read_csv_from_s3(self, s3_key: str) -> pd.DataFrame:
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
            return pd.read_csv(response['Body'])
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logging.error(f"Failed to read CSV from S3: NoSuchKey for {s3_key}")
            else:
                logging.error(f"Failed to read CSV from S3: {str(e)}")
            return pd.DataFrame()
        except Exception as e:
            logging.error(f"Unexpected error reading CSV from S3: {str(e)}")
            return pd.DataFrame()